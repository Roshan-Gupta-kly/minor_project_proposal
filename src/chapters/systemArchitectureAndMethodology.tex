\chapter{SYSTEM ARCHITECTURE AND METHODOLOGY}

\section{Proposed System Architecture}
\subsection{Proposed Sytem Block Diagram}
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{src/images/figures/block.jpg}
    \caption{Block Diagram of Proposed System Architecture}
    \label{fig:Block Diagram}
\end{figure}

\section{Working Principle}
The proposed system uses a combination of multisensor fusion and TinyML based event classification (Random Forest) to monitor vehicles in pre-event and post-event analysis in real-time that responds to potential accidents and alerts in case of any time of accident. The system is built around two microcontrollers effectively separated for one being for multifusion and another for verifying the accident parameters via ML model for either prediction or alert. 

The ESP32S3 Module I is responsible for acquisition of continuous data from multiple sensors. The multiple sensors includes accelerometer, gyroscope, magnetometer, a GPS module, a flame sensor, and a crash sensor respectively. These sensors collectively provide the information in a comprehensive manner regarding the motion, orientation, location and critical environment conditions of a vehicle at a given state. A sudden spike in a particular model is then forwarded into another module as by using decision inferences.
Multisensor fusion algorithm - Kalman Filter is used, where the algorithm integrates reading from all sensors to produce a robust and reliable estimation of state, minimization of the noise and transient errors from the individual sensors accordingly. The decision inference mechanism aforementioned deviates and forwards the data only that has significant deviations from normal operating conditions in the system. The relevant features of the potential abnormal events are extracted and summarized to the raw sensor reading into meaningly indicators such as acceleration magnitude, orientation angles, sudden speed changes, or activation of crash and flame sensors, which form the input for machine learning classification.

The ESP32S2 Module II is where our model performs to classify , normal, warning or an accident event. We use a Random forest model which performs TinyMl inference in our module. In the case of a normal event, the system continues monitoring without intervention. If the event is classified as a warning, the system triggers a local alert, activating a buzzer to notify the driver and allowing time to observe the situation before taking further action. When an accident is detected, the system immediately activates a GSM module to send an SOS message to predefined emergency contacts, ensuring timely assistance. A brief observation window is included to confirm persistent abnormal conditions before sending the SOS message, reducing the likelihood of false alarms.

Overall, the system combines real-time sensor monitoring, robust data fusion, and lightweight machine learning to provide a reliable, fast, and energy-efficient solution for intelligent vehicle accident detection. By integrating both preventive warnings and emergency communication, the system enhances vehicle safety while maintaining low computational requirements suitable for microcontroller-based embedded platforms.


\section{Kalman Filter Algorithm}

The Kalman Filter is an optimal recursive estimation algorithm that simplifies the complex computational process of data fusion by introducing assumptions regarding system dynamics, state evolution, and the statistical properties of noise and estimation errors \cite{kalman1960filtering}. It provides an efficient solution to the linear quadratic estimation problem for systems affected by stochastic disturbances.

The Kalman Filter formulates estimates of the internal state of a linear dynamic system by processing a sequence of noisy measurements. The system is assumed to be disturbed by zero-mean Gaussian white noise, and the filter recursively updates the state estimates as new measurements become available \cite{montella2011kalmanreview}. Owing to its recursive structure, the Kalman Filter is well suited for real-time applications such as sensor fusion, navigation, and vehicle safety systems.

The algorithm follows a predictor--corrector structure and can be divided into two distinct phases:
\begin{itemize}
    \item Time Update (Prediction)
    \item Measurement Update (Correction)
\end{itemize}

\subsection{Assumptions}

The Kalman Filter operates under the following assumptions \cite{kalman1960filtering}:

\begin{enumerate}
    \item The system dynamics are linear and described by the state transition equation:
    \begin{equation}
        \mathbf{s}_t = \mathbf{A}\mathbf{s}_{t-1} + \mathbf{B}\mathbf{u}_t + \mathbf{w}_t
    \end{equation}
    where $\mathbf{s}_t$ represents the system state vector, $\mathbf{u}_t$ denotes the control input, $\mathbf{A}$ is the state transition matrix, $\mathbf{B}$ is the control input matrix, and $\mathbf{w}_t$ is zero-mean Gaussian process noise with covariance $\mathbf{Q}$.

    \item The measurement model is linear and defined as:
    \begin{equation}
        \mathbf{z}_t = \mathbf{H}\mathbf{s}_t + \mathbf{v}_t
    \end{equation}
    where $\mathbf{z}_t$ is the measurement vector, $\mathbf{H}$ is the observation matrix, and $\mathbf{v}_t$ is zero-mean Gaussian measurement noise with covariance $\mathbf{R}$.

    \item The process and measurement noises are mutually independent and uncorrelated over time.
\end{enumerate}

\subsection{Time Update (Prediction Phase)}

In the time update phase, the Kalman Filter predicts the state of the system at the next time step using the system model \cite{durrantwhyte2006multisensor}.

\subsubsection*{State Prediction}
\begin{equation}
    \hat{\mathbf{s}}_{t|t-1} = \mathbf{A}\hat{\mathbf{s}}_{t-1|t-1} + \mathbf{B}\mathbf{u}_t
\end{equation}

\subsubsection*{Covariance Prediction}
\begin{equation}
    \mathbf{P}_{t|t-1} = \mathbf{A}\mathbf{P}_{t-1|t-1}\mathbf{A}^T + \mathbf{Q}
\end{equation}

After the prediction step, the prior estimate of the state is represented by a Gaussian distribution characterized by the mean $\hat{\mathbf{s}}_{t|t-1}$ and covariance $\mathbf{P}_{t|t-1}$.

\subsection{Measurement Update (Correction Phase)}

The measurement update phase corrects the predicted state using the current measurement, minimizing the estimation error covariance \cite{kalman1960filtering}.

\subsubsection*{Kalman Gain}
\begin{equation}
    \mathbf{K}_t = \mathbf{P}_{t|t-1}\mathbf{H}^T
    \left(
    \mathbf{H}\mathbf{P}_{t|t-1}\mathbf{H}^T + \mathbf{R}
    \right)^{-1}
\end{equation}

\subsubsection*{Innovation (Measurement Residual)}
\begin{equation}
    \mathbf{y}_t = \mathbf{z}_t - \mathbf{H}\hat{\mathbf{s}}_{t|t-1}
\end{equation}

\subsubsection*{State Update}
\begin{equation}
    \hat{\mathbf{s}}_{t|t} =
    \hat{\mathbf{s}}_{t|t-1} + \mathbf{K}_t \mathbf{y}_t
\end{equation}

\subsubsection*{Covariance Update}
\begin{equation}
    \mathbf{P}_{t|t} =
    \left(
    \mathbf{I} - \mathbf{K}_t \mathbf{H}
    \right)
    \mathbf{P}_{t|t-1}
\end{equation}

The updated state estimate and covariance represent the optimal linear unbiased estimate of the system state under the given assumptions, completing one iteration of the Kalman filtering process \cite{kalman1960filtering}.


\section{Random Forest Machine Learning Model}

A random forest is a classifier consisting of a collection of tree-structured classifiers 
$\{h(x,\Theta_k), k=1,\dots\}$ where the $\{\Theta_k\}$ are independent identically distributed 
random vectors and each tree casts a unit vote for the most popular class at input $x$~\cite{breiman2001random}.

The common element in all of these procedures is that for the $k$th tree, a random vector $\Theta_k$ 
is generated, independent of the past random vectors $\Theta_1, \dots, \Theta_{k-1}$ but with the same distribution; 
and a tree is grown using the training set and $\Theta_k$, resulting in a classifier $h(x,\Theta_k)$ 
where $x$ is an input vector. For instance, in bagging the random vector $\Theta$ is generated as the 
counts in $N$ boxes resulting from $N$ darts thrown at random at the boxes, where $N$ is the number 
of examples in the training set. In random split selection, $\Theta$ consists of a number of independent 
random integers between 1 and $K$. The nature and dimensionality of $\Theta$ depends on its use in tree construction. 
After a large number of trees is generated, they vote for the most popular class. These procedures are called random forests~\cite{scribd2025}.

\subsection{Random Forest Convergence}

Given an ensemble of classifiers $h_1(x),h_2(x),\dots,h_K(x)$, and with the training set drawn 
at random from the distribution of the random vector $(Y,X)$, define the margin function as:

\begin{equation}
\text{mg}(X,Y) = \text{avg}_k I(h_k(X)=Y) - \max_{j\neq Y} \text{avg}_k I(h_k(X)=j),
\end{equation}

where $I(\cdot)$ is the indicator function. The margin measures the extent to which the average 
number of votes at $(X,Y)$ for the correct class exceeds the average vote for any other class. 
The larger the margin, the more confidence in the classification. The generalization error is given by:

\begin{equation}
PE^* = P_{X,Y} (\text{mg}(X,Y) < 0),
\end{equation}

where the subscripts $X,Y$ indicate that the probability is over the $X,Y$ space.

In random forests, $h_k(X) = h(X, \Theta_k)$. For a large number of trees, it follows from the 
Strong Law of Large Numbers and the tree structure that:

\textbf{Theorem 1.} As the number of trees increases, for almost all sequences 
$\Theta_1, \dots$, $PE^*$ converges to

\begin{equation}
P_{X,Y}\Big(P_\Theta(h(X,\Theta)=Y) - \max_{j\neq Y} P_\Theta(h(X,\Theta)=j) < 0 \Big).
\end{equation}

This result explains why random forests do not overfit as more trees are added, but produce a limiting 
value of the generalization error~\cite{breiman2001random}.

\subsection{Key Features of Random Forests}

\begin{itemize}
    \item Its accuracy is as good as AdaBoost and sometimes better~\cite{breiman2001random}.
    \item It is relatively robust to outliers and noise~\cite{breiman2001random}.
    \item It is faster than bagging or boosting~\cite{breiman2001random}.
    \item Provides useful internal estimates of error, strength, correlation, and variable importance~\cite{breiman2001random}.
    \item Simple and easily parallelized~\cite{scribd2025}.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{src/images/figures/flowchart.jpg}
    \caption{Flowchart of Proposed System Architecture}
    \label{fig:flowchart}
\end{figure}
