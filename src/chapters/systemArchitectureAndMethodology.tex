\chapter{SYSTEM ARCHITECTURE AND METHODOLOGY}

\section{Proposed System Architecture}
\subsection{Proposed System Block Diagram}
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{src/images/figures/block.jpg}
    \caption{Block Diagram of Proposed System Architecture}
    \label{fig:Block Diagram}
\end{figure}

The block diagram shows us an accident detection and prediction system for vehicles. This system is really good at monitoring vehicles in time using edge computing. The accident detection and prediction system has two parts. Each part of the accident detection and prediction system uses an ESP32 S3 microcontroller. The ESP32 S3 microcontroller is very good at handling the intelligence tasks that the accident detection and prediction system needs.
In the beginning Module I is where all the information comes from. It takes in information from sensors that feel movement like the accelerometer, gyroscope and magnetometer. It also gets information from sensors that feel the environment, like the flame and crash sensors. At the time a GPS module keeps track of where the vehicle is. All this information is looked at by the Multisensor Fusion Algorithm. This algorithm helps get rid of any noise and puts all the different pieces of information together. This creates a picture of what is going on with the vehicle at that moment. The Multisensor Fusion Algorithm is very important for the vehicle. It helps the vehicles Module I make sense of all the information it gets from the motion sensors and environmental sensors. This information is then put together into events. This is better than using lots of pieces of data that do not make sense on their own. The machine learning model can understand events more easily than it can understand these small pieces of data. The machine learning model has a time with events.
The main brain of the system is in Module II, where a TinyML Inference engine does its job. This TinyML Inference engine runs a Random Forest model. The Random Forest model was trained on a computer. Then made smaller so it can run on the small hardware of the microcontroller. The system processes data on the device itself which is called the edge. This means the system does not have to wait for the cloud to respond, which is very important when something goes wrong like, during a crash. The TinyML Inference engine and the Random Forest model help the system make decisions quickly.


\section{Working Principle}
The proposed system uses a combination of multisensor fusion and Tiny\gls{ml}-based event classification (Random Forest) to monitor vehicles in pre-event and post-event analysis in real time, responding to potential accidents and generating alerts when necessary. The system is built around two microcontrollers, with one responsible for multisensor fusion and the other for verifying accident parameters via the \gls{ml} model for prediction or alert.

The \gls{esp32s3} Module I acquires continuous data from multiple sensors including \gls{acc}, \gls{gy}, \gls{ms}, \gls{gps}, \gls{fs}, and \gls{cs}. These sensors collectively provide comprehensive information regarding the motion, orientation, location, and critical environment conditions of a vehicle. Significant deviations in sensor data are forwarded to the second module using decision inference. 

A multisensor fusion algorithm based on the Kalman Filter is employed, integrating readings from all sensors to produce robust and reliable state estimation while minimizing noise and transient errors. Features of potential abnormal events are extracted from raw sensor readings, including acceleration magnitude, orientation angles, sudden speed changes, and activation of crash or flame sensors, forming the input for \gls{ml}-based classification.

The \gls{esp32s3} Module II performs the \gls{ml} inference to classify events as normal, warning, or accident. For normal events, the system continues monitoring without intervention. For warning events, the system triggers a local alert such as a buzzer. When an accident is detected, the system activates the \gls{gsm} module to send a \gls{sos} message to predefined emergency contacts. A brief observation window ensures persistent abnormal conditions are confirmed before sending \gls{sos} signals, reducing false alarms.

Overall, the system integrates real-time sensor monitoring, robust data fusion, and lightweight \gls{ml} inference to provide a reliable, fast, and energy-efficient solution for intelligent vehicle accident detection. Preventive warnings and emergency communication improve vehicle safety while maintaining low computational requirements suitable for microcontroller-based embedded platforms.

\section{Kalman Filter Algorithm}
The Kalman Filter is an optimal recursive estimation algorithm that simplifies the computational process of data fusion by introducing assumptions regarding system dynamics, state evolution, and the statistical properties of noise and estimation errors \cite{kalman1960filtering}. It provides an efficient solution to the linear quadratic estimation problem for systems affected by stochastic disturbances.

The Kalman Filter formulates estimates of the internal state of a linear dynamic system by processing a sequence of noisy measurements. The system is assumed to be disturbed by zero-mean Gaussian white noise, and the filter recursively updates the state estimates as new measurements become available \cite{montella2011kalmanreview}. Its predictor--corrector structure can be divided into two distinct phases:
\begin{itemize}
    \item Time Update (Prediction)
    \item Measurement Update (Correction)
\end{itemize}

\subsection{Assumptions}
The Kalman Filter operates under the following assumptions \cite{kalman1960filtering}:
\begin{enumerate}
    \item Linear system dynamics:
    \begin{equation}
        \mathbf{s}_t = \mathbf{A}\mathbf{s}_{t-1} + \mathbf{B}\mathbf{u}_t + \mathbf{w}_t
    \end{equation}
    \item Linear measurement model:
    \begin{equation}
        \mathbf{z}_t = \mathbf{H}\mathbf{s}_t + \mathbf{v}_t
    \end{equation}
    \item Process and measurement noises are mutually independent.
\end{enumerate}

\subsection{Time Update (Prediction Phase)}
\subsubsection*{State Prediction}
\begin{equation}
    \hat{\mathbf{s}}_{t|t-1} = \mathbf{A}\hat{\mathbf{s}}_{t-1|t-1} + \mathbf{B}\mathbf{u}_t
\end{equation}

\subsubsection*{Covariance Prediction}
\begin{equation}
    \mathbf{P}_{t|t-1} = \mathbf{A}\mathbf{P}_{t-1|t-1}\mathbf{A}^T + \mathbf{Q}
\end{equation}

\subsection{Measurement Update (Correction Phase)}
\subsubsection*{Kalman Gain}
\begin{equation}
    \mathbf{K}_t = \mathbf{P}_{t|t-1}\mathbf{H}^T
    \left(
    \mathbf{H}\mathbf{P}_{t|t-1}\mathbf{H}^T + \mathbf{R}
    \right)^{-1}
\end{equation}

\subsubsection*{Innovation (Measurement Residual)}
\begin{equation}
    \mathbf{y}_t = \mathbf{z}_t - \mathbf{H}\hat{\mathbf{s}}_{t|t-1}
\end{equation}

\subsubsection*{State Update}
\begin{equation}
    \hat{\mathbf{s}}_{t|t} =
    \hat{\mathbf{s}}_{t|t-1} + \mathbf{K}_t \mathbf{y}_t
\end{equation}

\subsubsection*{Covariance Update}
\begin{equation}
    \mathbf{P}_{t|t} =
    \left(
    \mathbf{I} - \mathbf{K}_t \mathbf{H}
    \right)
    \mathbf{P}_{t|t-1}
\end{equation}

\section{Random Forest Machine Learning Model}

A random forest is a classifier consisting of a collection of tree-structured classifiers 
$\{h(x,\Theta_k), k=1,\dots\}$ where the $\{\Theta_k\}$ are independent identically distributed 
random vectors and each tree casts a unit vote for the most popular class at input $x$~\cite{breiman2001random}.

The common element in all of these procedures is that for the $k$th tree, a random vector $\Theta_k$ 
is generated, independent of the past random vectors $\Theta_1, \dots, \Theta_{k-1}$ but with the same distribution; 
and a tree is grown using the training set and $\Theta_k$, resulting in a classifier $h(x,\Theta_k)$ 
where $x$ is an input vector. For instance, in bagging the random vector $\Theta$ is generated as the 
counts in $N$ boxes resulting from $N$ darts thrown at random at the boxes, where $N$ is the number 
of examples in the training set. In random split selection, $\Theta$ consists of a number of independent 
random integers between 1 and $K$. The nature and dimensionality of $\Theta$ depends on its use in tree construction. 
After a large number of trees is generated, they vote for the most popular class. These procedures are called random forests~\cite{scribd2025}.

\subsection{Random Forest Convergence}

Given an ensemble of classifiers $h_1(x),h_2(x),\dots,h_K(x)$, and with the training set drawn 
at random from the distribution of the random vector $(Y,X)$, define the margin function as:

\begin{equation}
\text{mg}(X,Y) = \text{avg}_k I(h_k(X)=Y) - \max_{j\neq Y} \text{avg}_k I(h_k(X)=j),
\end{equation}

where $I(\cdot)$ is the indicator function. The margin measures the extent to which the average 
number of votes at $(X,Y)$ for the correct class exceeds the average vote for any other class. 
The larger the margin, the more confidence in the classification. The generalization error is given by:

\begin{equation}
PE^* = P_{X,Y} (\text{mg}(X,Y) < 0),
\end{equation}

where the subscripts $X,Y$ indicate that the probability is over the $X,Y$ space.

In random forests, $h_k(X) = h(X, \Theta_k)$. For a large number of trees, it follows from the 
Strong Law of Large Numbers and the tree structure that:

\textbf{Theorem 1.} As the number of trees increases, for almost all sequences 
$\Theta_1, \dots$, $PE^*$ converges to

\begin{equation}
P_{X,Y}\Big(P_\Theta(h(X,\Theta)=Y) - \max_{j\neq Y} P_\Theta(h(X,\Theta)=j) < 0 \Big).
\end{equation}

This result explains why random forests do not overfit as more trees are added, but produce a limiting 
value of the generalization error~\cite{breiman2001random}.

\subsection{Key Features of Random Forests}

\begin{itemize}
    \item Its accuracy is as good as AdaBoost and sometimes better~\cite{breiman2001random}.
    \item It is relatively robust to outliers and noise~\cite{breiman2001random}.
    \item It is faster than bagging or boosting~\cite{breiman2001random}.
    \item Provides useful internal estimates of error, strength, correlation, and variable importance~\cite{breiman2001random}.
    \item Simple and easily parallelized~\cite{scribd2025}.
\end{itemize}


\section{Flowchart of Proposed System Architecture}
\begin{figure}[H]
    \centering
    \includegraphics[width=15cm]{src/images/figures/flowchart.jpg}
    \caption{Flowchart of Proposed System Architecture}
    \label{fig:flowchart}
\end{figure}

The system utilizes two ESP32 S3 modules to continuously monitor environment data, combining readings through a Multisensor Fusion Algorithm using the ESP32 S3 Module I to identify potential emergencies. When a significant sensor deviation is detected, the system extracts specific data features and employs a pre-trained Random Forest machine learning model done in the ESP32 S2 Module II to categorize the event as Normal, Warning, or Accident. While "Normal" events are ignored, a "Warning" triggers a buzzer and an "Accident" classification activates the GSM module. To prevent false alarms, the system implements a 5-second verification delay; if the situation does not return to normal within this window, it automatically sends an SOS message before ending the cycle. 
