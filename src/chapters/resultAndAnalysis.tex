\chapter{RESULTS AND ANALYSIS}

%  WRITING GUIDELINES FOR RESULTS AND ANALYSIS CHAPTER
%  ====================================================
The Results and Analysis chapter presents the outcomes of your project and critically analyzes
them. This chapter demonstrates whether your system meets requirements, compares performance
against objectives and baselines, and interprets findings. Results should be presented objectively
with appropriate visualizations, while analysis provides context and explanation. This chapter should be full of charts, tables, and figures illustrating your system's performance and behaviour.

%  STRUCTURE TEMPLATE:
\section{Experimental Configuration}
This section describes the experimental setup and parameter values used for evaluation. The configuration details are summarized in \cref{tab:experiment_config}.

\begin{table}[H]
    \centering
    \small
    \caption{Experimental Configuration Details}
    \begin{tabular}{lp{0.7\textwidth}}
        \toprule
        \textbf{Aspect}         & \textbf{Details}                                                                                                                        \\
        \midrule
        Hardware                & Describe the hardware platform used (e.g., microcontroller, sensors, computing device). Include specifications relevant to performance. \\
        Software                & List software tools, libraries, frameworks, and versions used in development and testing.                                               \\
        Environment             & Detail the physical or simulated environment where experiments were conducted (e.g., lab conditions, outdoor settings).                 \\
        Data Collection         & Explain how data was collected, including sampling rates, duration, and any preprocessing steps.                                        \\
        Evaluation Metrics      & Define the metrics used to assess performance (e.g., accuracy, latency, throughput, power consumption).                                 \\
        Experimental Procedures & Outline the steps taken during experiments, including any variations in conditions or configurations tested.                            \\
        \bottomrule
    \end{tabular}
    \label{tab:experiment_config}
\end{table}

\section{System Performance Evaluation}
The system performance was evaluated against the functional requirements established in Chapter 3. \Cref{tab:performance_comparison} compares the target specifications with the achieved results.

\begin{table}[H]
    \centering
    \small
    \caption{Performance Requirements vs. Achieved Results}
    \begin{tabular}{lccl}
        \toprule
        \textbf{Metric} & \textbf{Required} & \textbf{Achieved} & \textbf{Status} \\
        \midrule
        Accuracy        & $\geq 90\%$       & $94.2\%$          & \checkmark Met  \\
        Response Time   & $< 100$ ms        & $85$ ms           & \checkmark Met  \\
        Power Usage     & $< 500$ mW        & $420$ mW          & \checkmark Met  \\
        Uptime          & $> 24$ hours      & $32$ hours        & \checkmark Met  \\
        \bottomrule
    \end{tabular}
    \label{tab:performance_comparison}
\end{table}

\textit{Note: Replace the example metrics above with your actual project requirements and results.}
\section{Experimental Results}
This section presents the key experimental results obtained during system testing. \Cref{fig:performance_plot} shows the system performance over time under various operating conditions.

\begin{figure}[H]
    \centering
    % \includegraphics[width=0.8\textwidth]{path/to/your/plot.pdf}
    \fbox{\parbox{0.8\textwidth}{\centering\vspace{2cm}[Insert your performance plot here]\vspace{2cm}}}
    \caption{System performance metrics over time showing accuracy, response time, and resource utilization under different load conditions.}
    \label{fig:performance_plot}
\end{figure}

Additional quantitative results are summarized in \cref{tab:detailed_results}.

\begin{table}[H]
    \centering
    \small
    \caption{Detailed Experimental Results}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Test Scenario} & \textbf{Metric 1} & \textbf{Metric 2} & \textbf{Metric 3} & \textbf{Observations} \\
        \midrule
        Baseline Test          & 92.5\%            & 78 ms             & 380 mW            & Normal operation      \\
        High Load Test         & 89.3\%            & 95 ms             & 450 mW            & Slight degradation    \\
        Extended Duration      & 91.8\%            & 82 ms             & 390 mW            & Stable over 48h       \\
        \bottomrule
    \end{tabular}
    \label{tab:detailed_results}
\end{table}

\textit{Note: Replace placeholders with your actual experimental data, figures, and observations.}
\section{Performance Analysis}
\section{Comparison with Existing Solutions}
\section{Limitations and Trade-offs}

%  HOW TO WRITE - HARDWARE PROJECTS:
%  ----------------------------------
SYSTEM PERFORMANCE EVALUATION:
For each functional requirement identified earlier, present measured results showing whether it
was achieved. Use tables comparing required vs. achieved specifications: sensor accuracy, response
time, operating duration, communication range, power consumption, and other quantifiable metrics.
Be honest about requirements not fully met and explain why.

%  EXPERIMENTAL RESULTS:
Present systematically. If your hardware system performs measurements or control tasks, show
representative data: sensor readings over time, control system responses to inputs, communication
latency distributions, system behavior under various conditions.

Use appropriate visualizations:
%  - Time series plots for temporal data
%  - Bar charts for comparisons across conditions
%  - Box plots for distributions
%  - Scatter plots for relationships between variables

For systems with multiple operating modes or scenarios, dedicate subsections to each. For example,
environmental monitoring system: "Indoor Performance," "Outdoor Performance," "Extreme Temperature
%  Testing." For each scenario, present data, analyze performance, discuss factors affecting results.

%  PERFORMANCE ANALYSIS:
Interpret results. Explain why certain performance levels were achieved—"The sensor accuracy of 95\
exceeds the 90\ requirement due to the Kalman filtering implementation described in Section 4.3."
Discuss performance variations—"Accuracy decreases to 87\ in high-humidity conditions because
moisture affects sensor readings." Relate findings to literature—"Our system's power consumption
of 50mW is 30\ lower than the comparable system by Smith et al. [15] due to our adaptive sampling
strategy."

ML MODEL PERFORMANCE (if applicable):
Present: accuracy, precision, recall, F1-scores on test dataset, confusion matrices, inference
time measurements on actual hardware, memory usage during inference, comparison with non-ML
baselines. Analyze when model performs well vs. poorly—"The model achieves 98\ accuracy for
stationary objects but only 82\ for moving objects due to motion blur in the camera."

SYSTEM INTEGRATION ANALYSIS:
How well subsystems work together, any bottlenecks identified, timing analysis of complete
sense-process-act cycle, communication reliability statistics, system stability over extended
operation. If you conducted stress testing, present results showing system behavior under extreme
conditions.

%  COMPARISON WITH EXISTING SOLUTIONS:
Create comparison table showing your system alongside similar systems from literature or commercial
products. Compare on relevant dimensions: performance metrics, cost, power consumption, features,
%  limitations. Be objective—acknowledge where others excel and where your system excels. Quantify
improvements: "Our system reduces cost by 60\ while maintaining comparable accuracy."

%  LIMITATIONS AND TRADE-OFFS:
No system is perfect—acknowledging limitations demonstrates scientific maturity. Explain trade-offs
made: "We prioritized battery life over processing speed, resulting in 48-hour operation but
2-second response time. For applications requiring faster response, this trade-off could be reversed."

Include visualizations: performance plots (line charts for time series, bar charts for comparisons),
system behavior under different conditions, confusion matrices for ML components, resource usage
charts (CPU, memory, power), comparison tables with other systems, photographs of physical system
in operation.

%  HOW TO WRITE - ML/SOFTWARE PROJECTS:
%  -------------------------------------
OVERALL PERFORMANCE:
Present primary metrics on test set:
%  - Classification: accuracy, precision, recall, F1-score
%  - Regression: MAE, RMSE, R²
%  - Language models: perplexity
%  - Object detection: mAP
Present in clear table with confidence intervals or standard deviations if you performed multiple
runs with different random seeds.

DETAILED PERFORMANCE ANALYSIS:
For classification:
%  - Confusion matrices with analysis of which classes are most confused and why
%  - Precision-recall curves
%  - ROC curves with AUC values
%  - Per-class performance breakdown

For regression:
%  - Residual plots showing error distributions
%  - Scatter plots of predicted vs. actual values
%  - Analysis of where large errors occur

Identify patterns—"The model struggles with minority class examples, achieving only 73\ recall
compared to 94\ for majority classes."

COMPARATIVE ANALYSIS:
Compare your model against baselines (simple heuristics, traditional ML methods, existing published
results). Use tables and bar charts for clear comparison. Quantify improvements: "Our model achieves
4.2\ higher F1-score than the previous state-of-the-art on this dataset." Perform statistical
significance tests (t-tests, McNemar's test) to determine if improvements are statistically significant.

ABLATION STUDIES:
Show contribution of different components. For example, if you used data augmentation, ensemble
methods, and attention mechanisms, show performance with each component removed. This demonstrates
which innovations actually contributed to performance gains and provides insights for future work.

MODEL BEHAVIOR ANALYSIS:
Analyze through qualitative examples:
%  - Image classification: show correctly classified and misclassified examples with explanations
%  - NLP tasks: show sample predictions with model's confidence scores
%  - Interpretable models: present feature importance rankings or attention visualizations
This qualitative analysis helps readers understand what the model has learned and its failure modes.

GENERALIZATION ANALYSIS (if relevant):
%  Performance on out-of-domain data, cross-dataset evaluation, performance across different demographic
groups (fairness analysis), robustness to input perturbations or adversarial examples. This is
increasingly important for real-world deployment.

EFFICIENCY ANALYSIS:
%  Training time, convergence curves, inference latency (mean and percentiles), memory footprint,
model size, throughput (samples per second). Compare against competing approaches—"Our model
achieves comparable accuracy but with 3× faster inference time, enabling real-time applications."

SOFTWARE FUNCTIONAL VALIDATION:
Demonstrate that each major feature works as specified through screenshots, usage scenarios, or
recorded demonstrations. Use tables to check off each functional requirement and indicate whether
it was fully implemented, partially implemented, or deferred.

%  PERFORMANCE RESULTS (Software):
%  - Response time measurements (page load times, API latency)
%  - Throughput tests (requests per second, concurrent users supported)
%  - Resource utilization (CPU, memory, database query times)
%  - Scalability analysis (performance degradation as load increases)
Present with charts showing response time distributions, throughput vs. load curves, resource
usage under different conditions.

USER EXPERIENCE ANALYSIS (if applicable):
Usability testing results, user satisfaction surveys, task completion times, error rates during
user interactions. For commercial or research prototypes, actual user feedback is valuable.

%  COMPARISON WITH ALTERNATIVES:
How your software compares with existing tools, frameworks, or products in functionality,
%  performance, ease of use, cost, or other relevant dimensions. Be balanced and objective.

%  LIMITATIONS:
%  Performance bottlenecks identified, scalability limits, features not implemented, edge cases not
handled, security considerations, browser compatibility issues, dependencies on third-party services.
Explain impact of these limitations and potential mitigation strategies.

Include visualizations: performance plots, confusion matrices and ROC curves for ML, UI screenshots
annotated with features, system architecture diagrams showing deployed components, monitoring
dashboards showing system health, comparison bar charts and tables, qualitative visualizations
(attention maps, sample outputs, error examples).

%  LaTeX quick refs:
%  - Always reference figures/tables in text before they appear.
%  - Use cleveref: \cref{fig:...} or \Cref{tab:...} for automatic formatting
%  - Keep captions descriptive; labels unique per chapter.